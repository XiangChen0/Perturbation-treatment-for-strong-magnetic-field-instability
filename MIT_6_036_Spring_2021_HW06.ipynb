{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 Spring 2021 HW06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiangChen0/Perturbation-treatment-for-strong-magnetic-field-instability/blob/main/MIT_6_036_Spring_2021_HW06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Spring 2021: Homework 06#\n",
        "\n",
        "This colab notebook provides code and a framework for [Homework 06](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2021_Spring/courseware/Week6/week6_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "**Note**: You can go to `File > Save a copy in Drive...` to save your own copy of this notebook for editing.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "outputId": "2a4e6ca6-3907-4a71-ec90-7a276c7d2ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf code_for_hw6*\n",
        "!rm -rf mnist\n",
        "!rm -rf data\n",
        "!wget --no-check-certificate --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw06/code_for_hw6.zip\n",
        "!unzip code_for_hw6.zip\n",
        "!mv code_for_hw6/* .\n",
        "\n",
        "from code_for_hw6 import *\n",
        "import numpy as np\n",
        "import modules_disp as disp\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw6.zip\n",
            "   creating: code_for_hw6/\n",
            "  inflating: code_for_hw6/.DS_Store  \n",
            "  inflating: __MACOSX/code_for_hw6/._.DS_Store  \n",
            "  inflating: code_for_hw6/modules_disp.py  \n",
            "  inflating: code_for_hw6/utils_hw6.py  \n",
            "  inflating: code_for_hw6/code_for_hw6.py  \n",
            "   creating: code_for_hw6/data/\n",
            "  inflating: code_for_hw6/expected_results.py  \n",
            "  inflating: code_for_hw6/data/data5_train.csv  \n",
            "  inflating: code_for_hw6/data/data3_train.csv  \n",
            "  inflating: code_for_hw6/data/data4_train.csv  \n",
            "  inflating: code_for_hw6/data/data4_validate.csv  \n",
            "  inflating: code_for_hw6/data/data3_validate.csv  \n",
            "  inflating: code_for_hw6/data/dataXor_train.csv  \n",
            "  inflating: code_for_hw6/data/data2_train.csv  \n",
            "  inflating: code_for_hw6/data/data2_validate.csv  \n",
            "  inflating: code_for_hw6/data/data5_validate.csv  \n",
            "  inflating: code_for_hw6/data/data3class_train.csv  \n",
            "  inflating: code_for_hw6/data/data1_validate.csv  \n",
            "  inflating: code_for_hw6/data/data1_train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFxhrJ5XDlvb"
      },
      "source": [
        "# Implementing Neural Networks\n",
        "\n",
        "This problem follows directly from the **Backpropagation** problem in [Homework 05](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2021_Spring/courseware/Week5/week5_homework/). Make sure that you have done and understood that problem before beginning this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjQgtwPHj08n"
      },
      "source": [
        "Although for \"real\" applications you want to use one of the many packaged implementations of neural networks (we'll start using one of those soon), there is no substitute for implementing one yourself to get an in-depth understanding. Luckily, that is relatively easy to do if we're not too concerned with maximum efficiency.\n",
        "\n",
        "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
        "\n",
        "```\n",
        "# build a 3-layer network\n",
        "net = Sequential([Linear(2,3), Tanh(),\n",
        "                  Linear(3,3), Tanh(),\n",
        "    \t          Linear(3,2), SoftMax()])\n",
        "# train the network on data and labels\n",
        "net.sgd(X, Y)\n",
        "```\n",
        "We call this a Sequential object because we feed the outputs of the previous layer into the next, you could say this forms a sequential relationship between layers.\n",
        "\n",
        "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwpgsbnho9K"
      },
      "source": [
        "## 1) Linear Modules: ##\n",
        "Each linear module has a forward method that takes in a batch of activations $A$ (from the previous layer) and returns a batch of pre-activations $Z$.\n",
        "\n",
        "Each linear module has a backward method that takes in `dLdZ` and returns `dLdA`. This module also computes and stores `dLdW` and `dLdW0`, the gradients with respect to the weights.\n",
        "\n",
        "Hint: be careful with dimensions when computing `dLdW0`. `dLdZ` is $(n \\times b)$, but `dLdW0` is $(n \\times 1)$. Why do you need to sum over all $b$ points in the batch when computing `dLdW0`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VsYLAxCfy7U"
      },
      "source": [
        "class Module:\n",
        "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        # initializes the weights randomly and offsets as 0\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "    def forward(self, A):\n",
        "        # store the input matrix for future use\n",
        "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
        "        return np.dot(self.W.T, A) + self.W0  # Your code (n x b)\n",
        "\n",
        "    def backward(self, dLdZ):  \n",
        "        # dLdZ is (n x b), uses stored self.A\n",
        "        # store the derivatives for use in sgd_step and return dLdA\n",
        "        self.dLdW = np.dot(self.A, dLdZ.T)       # Your code\n",
        "        self.dLdW0 = np.dot(dLdZ, np.ones([dLdZ.shape[1],1]))     # Your code\n",
        "        return np.dot(self.W, dLdZ)        # Your code: return dLdA (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W = self.W - lrate * self.dLdW          # Your code\n",
        "        self.W0 = self.W0 - lrate * self.dLdW0       # Your code\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqZ7_kZYr5s5"
      },
      "source": [
        " You are **highly encouraged** to make your own tests for each module. The test cases being run on catsoop are given below for your reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3yePY0r4eA"
      },
      "source": [
        "def nn_linear_forward():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    return linear.forward(X).tolist()\n",
        "\n",
        "expected_linear_forward = [[10.417500637754383, 6.911221682745654, 20.733665048236965, 22.891234399772113], [7.168722346625092, 3.489987464919749, 10.469962394759248, 9.998261102396512], [-2.071054548689073, 0.6941371647696142, 2.0824114943088414, 4.849668106971125]]\n",
        "print(\"Pass linear forward? \", np.allclose(nn_linear_forward(), expected_linear_forward))\n",
        "\n",
        "def nn_linear_forward_bias():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    linear.W0 = np.array([[1],[1],[1]])\n",
        "    X,Y = super_simple_separable()\n",
        "    return linear.forward(X).tolist()\n",
        "\n",
        "expected_linear_forward_bias = [[11.417500637754383, 7.911221682745654, 21.733665048236965, 23.891234399772113], [8.168722346625092, 4.489987464919749, 11.469962394759248, 10.998261102396512], [-1.071054548689073, 1.6941371647696142, 3.0824114943088414, 5.849668106971125]]\n",
        "print(\"Pass linear forward bias? \", np.allclose(nn_linear_forward_bias(), expected_linear_forward_bias))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2oTBRYETrI_"
      },
      "source": [
        "def nn_linear_backward():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    return linear.backward(dLdZ).tolist()\n",
        "\n",
        "expected_linear_backward = [[3.889497924054116, 1.247373376201773, 0.2829538755771419, 0.6920722655660196], [2.1525571673658237, 1.5845507770701677, 1.3205629190941617, -0.6910398159642225]]\n",
        "print(\"Pass linear backward? \", np.allclose(nn_linear_backward(), expected_linear_backward))\n",
        "\n",
        "def nn_linear_backward_stored_dLdW_dLdW0():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    linear.backward(dLdZ)\n",
        "    return [linear.dLdW.tolist(), linear.dLdW0.tolist()]\n",
        "\n",
        "expected_linear_backward_stored = [[[5, 13, 18], [7, 16, 20]], [[2], [3], [4]]]\n",
        "print(\"Pass linear backward stored vals dLdW? \", np.allclose(nn_linear_backward_stored_dLdW_dLdW0()[0], expected_linear_backward_stored[0]))\n",
        "print(\"Pass linear backward stored vals dLdW0? \", np.allclose(nn_linear_backward_stored_dLdW_dLdW0()[1], expected_linear_backward_stored[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCBJbR5L8p3s"
      },
      "source": [
        "def nn_linear_sgd():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    linear.backward(dLdZ)\n",
        "    linear.sgd_step(0.005)\n",
        "    return [np.vstack([linear.W, linear.W0.T]).tolist()]\n",
        "\n",
        "expected_linear_sgd = [[[1.222373376201773, 0.2179538755771419, 0.6020722655660197], [1.5495507770701678, 1.2405629190941616, -0.7910398159642225], [-0.01, -0.015, -0.02]]]\n",
        "print(\"Pass linear sgd? \", np.allclose(nn_linear_sgd(), expected_linear_sgd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETL01mPsBz4"
      },
      "source": [
        "The following datasets are defined for your use:\n",
        "*  `super_simple_separable_through_origin()`\n",
        "*  `super_simple_separable()`\n",
        "*  `xor()`\n",
        "*  `xor_more()`\n",
        "*  `hard()`\n",
        "\n",
        "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
        "```\n",
        "def plot_nn(X, Y, nn):\n",
        "    \"\"\" Plot output of nn vs. data \"\"\"\n",
        "    def predict(x):\n",
        "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
        "    xmin, ymin = np.min(X, axis=1)-1\n",
        "    xmax, ymax = np.max(X, axis=1)+1\n",
        "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
        "    plot_data(X, Y, nax)\n",
        "    plt.show()```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s70beWJh09h"
      },
      "source": [
        "## 2) Activation functions: ##\n",
        "Each activation module has a forward method that takes in a batch of pre-activations $Z$ and returns a batch of activations $A$. (Ask yourself, why is this the reverse of the linear module?)\n",
        "\n",
        "Each activation module has a backward method that takes in `dLdA` and returns `dLdZ`, with the exception of SoftMax, where we assume `dLdZ` is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwaNAtLnhenT"
      },
      "source": [
        "### 2.1) Tanh: ###\n",
        "Please use `np.tanh` here. You can find documentation online.\n",
        "\n",
        "Hint 1: the derivative of $\\tanh$ is given by $\\frac{d\\tanh(z)}{d z} = 1 - \\tanh(z)^2$. \n",
        "\n",
        "Hint 2: If you're having problems with this, try writing out the shapes of everything involved and remember that `@` does matrix multiplication while `*` does component-wise multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6eD3dnftiR"
      },
      "source": [
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)            # Your code\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return dLdA*(1-self.A*self.A)            # Your code: return dLdZ"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpo88_6wQh0R"
      },
      "source": [
        "Tanh unit test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZV85HbJQlhc"
      },
      "source": [
        "def nn_tanh_forward():\n",
        "    np.random.seed(0)\n",
        "    tanh = Tanh()\n",
        "    X,Y = super_simple_separable()\n",
        "    return tanh.forward(X).tolist()\n",
        "\n",
        "expected_tanh_forward = [[0.9640275800758169, 0.9950547536867305, 0.999999969540041, 0.9999999999244973], [0.9999092042625951, 0.9640275800758169, 0.9999877116507956, 0.9999092042625951]]\n",
        "print(\"Pass tanh forward? \", np.allclose(nn_tanh_forward(), expected_tanh_forward))\n",
        "\n",
        "def nn_tanh_backward():\n",
        "    np.random.seed(0)\n",
        "    tanh = Tanh()\n",
        "    X,Y = super_simple_separable()\n",
        "    tanh.forward(X)\n",
        "    dLdA = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0]])\n",
        "    return tanh.backward(dLdA).tolist()\n",
        "\n",
        "expected_tanh_backward = [[0.07065082485316443, 0.009866037165440211, 0.0, 0.0], [0.0003631664618877206, 0.0, 2.4576547405286142e-05, 0.0]]\n",
        "print(\"Pass tanh backward? \", np.allclose(nn_tanh_backward(), expected_tanh_backward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FW7ocKRhcgY"
      },
      "source": [
        "###2.2) ReLU: ###\n",
        "Hint:\n",
        "[`np.maximum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html) might be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fm2KsLUfqdp"
      },
      "source": [
        "class ReLU(Module):              # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.maximum(Z,np.zeros(Z.shape))            # Your code\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # uses stored self.A\n",
        "        return dLdA * np.sign(self.A)         # Your code: return dLdZ"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMxtp-BJRyus"
      },
      "source": [
        "ReLU unit tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9NDufA7R16i"
      },
      "source": [
        "def nn_relu_forward():\n",
        "    np.random.seed(0)\n",
        "    relu = ReLU()\n",
        "    X,Y = super_simple_separable()\n",
        "    return relu.forward(X).tolist()\n",
        "\n",
        "expected_relu_forward = [[2, 3, 9, 12], [5, 2, 6, 5]]\n",
        "print(\"Pass relu forward? \", np.allclose(nn_relu_forward(), expected_relu_forward))\n",
        "\n",
        "def nn_relu_backward():\n",
        "    np.random.seed(0)\n",
        "    relu = ReLU()\n",
        "    X,Y = super_simple_separable()\n",
        "    relu.forward(X)\n",
        "    dLdA = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0]])\n",
        "    return relu.backward(dLdA).tolist()\n",
        "\n",
        "expected_relu_backward = [[1, 1, 0, 0], [2, 0, 1, 0]]\n",
        "print(\"Pass relu backward? \", np.allclose(nn_relu_backward(), expected_relu_backward))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKtXuTQ0hSNO"
      },
      "source": [
        "###2.3) SoftMax: ###\n",
        "We will now implement the softmax activation function. Note that we will assume that the derivative `dLdZ` is passed into the backward method. We also have the additional `class_fun` method:\n",
        "\n",
        "* `SoftMax.class_fun`: Given `Ypred`, a matrix where each column corresponds to a data point's vector of class probabilities (computed by Softmax), this returns a vector of the classes (integers) with the highest probability for each point. The vector should be 1-dimensional with shape `(b,)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqK-CJrnfn22"
      },
      "source": [
        "class SoftMax(Module):           # Output activation\n",
        "    def forward(self, Z):\n",
        "        return np.exp(Z)/np.sum(np.exp(Z),axis=0)              # Your code\n",
        "\n",
        "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred):  \n",
        "        # Returns the index of the most likely class for each point as vector of shape (b,)\n",
        "        return np.argmax(Ypred, axis=0)              # Your code\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DrQDUepW5gd"
      },
      "source": [
        "Test cases for softmax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-7HrY_4W7yW"
      },
      "source": [
        "def nn_softmax_forward():\n",
        "    np.random.seed(0)\n",
        "    softmax = SoftMax()\n",
        "    X,Y = super_simple_separable()\n",
        "    return softmax.forward(X).tolist()\n",
        "\n",
        "expected_softmax_forward = [[0.04742587317756679, 0.7310585786300048, 0.9525741268224334, 0.9990889488055993], [0.9525741268224333, 0.2689414213699951, 0.04742587317756678, 0.0009110511944006454]]\n",
        "print(\"Pass softmax forward? \", np.allclose(nn_softmax_forward(), expected_softmax_forward))\n",
        "  \n",
        "def nn_softmax_class_fun():\n",
        "    np.random.seed(0)\n",
        "    softmax = SoftMax()\n",
        "    X,Y = super_simple_separable()\n",
        "    Ypred = softmax.forward(X)\n",
        "    return softmax.class_fun(Ypred).tolist()\n",
        "\n",
        "expected_softmax_class_fun = [1, 0, 0, 0]\n",
        "print(\"Pass softmax class fun? \", np.allclose(nn_softmax_class_fun(), expected_softmax_class_fun))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZc7HnMSh4fn"
      },
      "source": [
        "## 3) Loss Functions:##\n",
        "We will now implement the NLL loss function assuming that the output activation function is softmax and using the definition from homework 5:\n",
        "\n",
        "$$NLL(y^{pred}, y) = -\\sum_{j = 1}^{n^L} y_j \\ln y^{pred}_j.$$\n",
        "\n",
        "Hint 1: since we are using Softmax $ = SM(Z)$ at the output layer and\n",
        " _NLL_ as the $Loss(A, Y)$ function, we have seen that there is a\n",
        "simple form for ${\\tt dLdZ} = \\frac{\\partial Loss}{\\partial Z}$;\n",
        "namely, it is the prediction error $Y^{pred} - Y$ (check HW 6 for a refresher).  A similar result holds\n",
        "when using _NLL_ with a sigmoid output activation or a quadratic loss\n",
        "with a linear output activation.\n",
        "\n",
        "Hint 2: Note that `np.log` might be helpful. You can find documentation online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Fb8mimflgb"
      },
      "source": [
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        # returns loss as a float\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return -np.sum(Y*np.log(Ypred))      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        # note, this is the derivative of loss with respect to the input of softmax\n",
        "        return self.Ypred-self.Y     # Your code\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhL4QL5xYqYe"
      },
      "source": [
        "NLL Test Cases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB6Y0Tl5Ys7v"
      },
      "source": [
        "def nn_nll_forward():\n",
        "    nll = NLL()\n",
        "    y = np.array([[1, 0, 1, 0]])\n",
        "    Y = for_softmax(y)\n",
        "    ypred = np.array([[.7, .3, .99, .99]])\n",
        "    Ypred = for_softmax(ypred)\n",
        "    return nll.forward(Ypred, Y)\n",
        "\n",
        "expected_nll_forward = 5.328570409719057\n",
        "print(\"Pass nll forward? \", np.allclose(nn_nll_forward(), expected_nll_forward))\n",
        "\n",
        "def nn_nll_backward():\n",
        "    nll = NLL()\n",
        "    y = np.array([[1, 0, 1, 0]])\n",
        "    Y = for_softmax(y)\n",
        "    ypred = np.array([[.7, .3, .99, .99]])\n",
        "    Ypred = for_softmax(ypred)\n",
        "    nll.forward(Ypred, Y)\n",
        "    return nll.backward().tolist()\n",
        "\n",
        "expected_nll_backward = [[0.30000000000000004, -0.30000000000000004, 0.010000000000000009, -0.99], [-0.30000000000000004, 0.3, -0.010000000000000009, 0.99]]\n",
        "print(\"Pass nll backward? \", np.allclose(nn_nll_backward(), expected_nll_backward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EffzDFkqMX"
      },
      "source": [
        "## Activation and Loss Test Cases: ##\n",
        "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DJFzpahkvcD"
      },
      "source": [
        "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0dXg-Qk05_"
      },
      "source": [
        "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5JgBU2iBCZ"
      },
      "source": [
        "## 4) Sequential ##\n",
        "\n",
        "Now we will implement SGD following these specs:\n",
        "\n",
        "* Randomly pick a data point `Xt`, `Yt` by using `np.random.randint` to choose a random index into the data.\n",
        "Compute the predicted output `Ypred` for `Xt` with the forward method.\n",
        "* Compute the loss for `Ypred` relative to `Yt`.\n",
        "* Use the backward method to compute the gradients. (Hint: where should backprop start? Which module has a backward that takes no input?)\n",
        "* Use the `sgd_step` method to change the weights.\n",
        "* Repeat.\n",
        "\n",
        "For context, we can construct a network and train it as follows:\n",
        "\n",
        "```\n",
        "# build a 3-layer network\n",
        "net = Sequential([Linear(2,3), Tanh(),\n",
        "                  Linear(3,3), Tanh(),\n",
        "                  Linear(3,2), SoftMax()], NLL())\n",
        "# train the network on data and labels\n",
        "net.sgd(X, Y)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejO15Vr7fhKB"
      },
      "source": [
        "class Sequential:\n",
        "    def __init__(self, modules, loss):            # List of modules, loss module\n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
        "        D, N = X.shape\n",
        "        for it in range(iters):\n",
        "            rdn = np.random.randint(N) # YOUR CODE HERE\n",
        "            Xt = X[:,rdn:rdn+1]\n",
        "            Yt = Y[:,rdn:rdn+1]\n",
        "            Ypred = self.forward(Xt)\n",
        "            cur_loss = self.loss.forward(Ypred, Yt)\n",
        "            self.print_accuracy(it, X, Y, cur_loss, every=1)\n",
        "            delta = self.loss.backward()\n",
        "            self.backward(delta)\n",
        "            self.sgd_step(lrate)\n",
        "            \n",
        "    def forward(self, Xt):                        # Compute Ypred\n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
        "        # Note reversed list of modules\n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):                    # Gradient descent step\n",
        "        for m in self.modules: m.sgd_step(lrate)\n",
        "\n",
        "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
        "        # Utility method to print accuracy on full dataset, should\n",
        "        # improve over time when doing SGD. Also prints current loss,\n",
        "        # which should decrease over time. Call this on each iteration\n",
        "        # of SGD!\n",
        "        if it % every == 1:\n",
        "            cf = self.modules[-1].class_fun\n",
        "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
        "            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)\n",
        "            \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUojaXqphDjh"
      },
      "source": [
        "\n",
        "Use Test 3 to help you debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWfgC7Qe3ar"
      },
      "source": [
        "# TEST 3: try calling these methods that train with a simple dataset\n",
        "from pprint import pprint\n",
        "\n",
        "tanh_expected = [np.array([[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
        "                           [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
        "                           [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]]),\n",
        "                 np.array([[0.544808855557535, -0.08366117689965663],\n",
        "                           [-0.06331837550937103, 0.24078409926389266],\n",
        "                           [0.08677202043839037, 0.8360167748667923],\n",
        "                           [-0.0037249480614717995, 0.0037249480614718]])]\n",
        "\n",
        "relu_expected = [np.array( [[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
        "                            [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
        "                            [-0.002775491757223511, 0.001212351486908601, -0.0005239629389906042]]),\n",
        "                 np.array([[0.501769700845158, -0.04062202218727964],\n",
        "                           [-0.09260786974986725, 0.27007359350438886],\n",
        "                           [0.08364438851530624, 0.8391444067898763],\n",
        "                           [-0.004252310922204505, 0.004252310922204505]])]\n",
        "\n",
        "pred_expected = [np.array([0, 0, 0, 0]),\n",
        "                 np.array([8.565750618357672])]\n",
        "\n",
        "def nn_tanh_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    out = [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "    print(\"Output was:\")\n",
        "    pprint(out)\n",
        "    print(\"Passed?\", np.allclose(out[0], tanh_expected[0]) and np.allclose(out[1], tanh_expected[1]))    \n",
        "\n",
        "\n",
        "def nn_relu_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
        "    out = [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "    print(\"Output was:\")\n",
        "    pprint(out)\n",
        "    print(\"Passed?\", np.allclose(out[0], relu_expected[0]) and np.allclose(out[1], relu_expected[1]))\n",
        "\n",
        "\n",
        "def nn_pred_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    Ypred = nn.forward(X)\n",
        "    out = nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n",
        "    print(\"Output was:\")\n",
        "    pprint(out)\n",
        "    print(\"Passed?\", np.allclose(out[0], pred_expected[0]) and np.allclose(out[1], pred_expected[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx-zM2y3R0z"
      },
      "source": [
        "nn_tanh_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
        "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
        "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
        "#  [[0.544808855557535, -0.08366117689965663],\n",
        "#   [-0.06331837550937104, 0.24078409926389266],\n",
        "#   [0.08677202043839037, 0.8360167748667923],\n",
        "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
        "# '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYT9IWk3TQL"
      },
      "source": [
        "nn_relu_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
        "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
        "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
        "#  [[0.501769700845158, -0.040622022187279644],\n",
        "#   [-0.09260786974986723, 0.27007359350438886],\n",
        "#   [0.08364438851530624, 0.8391444067898763],\n",
        "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
        "# '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_woDFh3a2v"
      },
      "source": [
        "nn_pred_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# ([0, 0, 0, 0], [8.56575061835767])\n",
        "# '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZAlVn14TV4Y",
        "outputId": "5236738a-2211-44ea-e7d7-9cfc6a88dff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Sigmoid(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = 1/(1+np.exp(-Z))            # Your code\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return dLdA*self.A*(1-self.A)            # Your code: return dLdZ\n",
        "\n",
        "class Quad(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = Z*Z \n",
        "        self.Z = Z        # Your code\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return dLdA* 2 * self.Z\n",
        "\n",
        "class MSE(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        # returns loss as a float\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return np.sum((Ypred-Y)*(Ypred-Y))     # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        # note, this is the derivative of loss with respect to the input of softmax\n",
        "        return 2*(self.Ypred-self.Y)    # Your code\n",
        "\n",
        "net = Sequential([Linear(2,4), ReLU(), Linear(4,1), Sigmoid()], MSE())\n",
        "# train the network on data and labels\n",
        "X= np.array([[-1.5,1.5],[0,1.5],[1.5,1.5],[1.5,0],[1.5,-1.5],[0,-1.5],[-1.5,-1.5],[-1.5,0],[-0.5,-0.5],[-0.5,0.5],[0.5,0.5],[0.5,-0.5]]).T\n",
        "Y = np.array([[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0]]).T\n",
        "net.sgd(X, Y, iters=100000, lrate=0.1)\n",
        "Ypred = net.forward(X)\n",
        "print(Ypred)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.9903569  0.99999999 0.99061392 0.9964275  0.9964511\n",
            "  0.99647453 0.99034348 0.00571494 0.01522654 0.01514916 0.00568981]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjuYP0AJZzJ-",
        "outputId": "335c272f-2b8a-4a14-a785-a2c950b052ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(net.modules[0].W)\n",
        "print(net.modules[0].W0)\n",
        "print(net.modules[2].W)\n",
        "print(net.modules[2].W0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.19300728  2.34246014 -2.68816739  0.11681732]\n",
            " [ 2.82429688  0.20660751  2.83751282  2.28589481]]\n",
            "[[-0.31557096]\n",
            " [-1.06794027]\n",
            " [-0.23632433]\n",
            " [ 3.42894123]]\n",
            "[[ 3.53396985]\n",
            " [ 2.39532171]\n",
            " [ 3.67909773]\n",
            " [-4.0472499 ]]\n",
            "[[4.21657051]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoobdT7tbUaw",
        "outputId": "f030af92-42ba-401f-9be7-557e7a6a7862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = Sequential([Linear(2,2), Quad(), Linear(2,1), Sigmoid()], MSE())\n",
        "# train the network on data and labels\n",
        "X= np.array([[-1.5,1.5],[0,1.5],[1.5,1.5],[1.5,0],[1.5,-1.5],[0,-1.5],[-1.5,-1.5],[-1.5,0],[-0.5,-0.5],[-0.5,0.5],[0.5,0.5],[0.5,-0.5]]).T\n",
        "Y = np.array([[1],[1],[1],[1],[1],[1],[1],[1],[0],[0],[0],[0]]).T\n",
        "net.sgd(X, Y, iters=100000, lrate=0.1)\n",
        "Ypred = net.forward(X)\n",
        "print(Ypred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99999997 0.99407165 0.99999997 0.99392864 0.99999997 0.99416746\n",
            "  0.99999997 0.99399073 0.0128898  0.01276258 0.01277692 0.01278801]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8woWs7kbirQ",
        "outputId": "d3f3eb79-21b5-4b45-c5cd-17b8809e97a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(net.modules[0].W)\n",
        "print(net.modules[0].W0)\n",
        "print(net.modules[2].W)\n",
        "print(net.modules[2].W0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.86438876  1.87954685]\n",
            " [ 1.83603221  0.88808201]]\n",
            "[[-0.00065058]\n",
            " [-0.00104702]]\n",
            "[[1.31297007]\n",
            " [1.25110123]]\n",
            "[[-7.04847998]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}